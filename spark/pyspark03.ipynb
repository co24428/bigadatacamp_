{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare and understand data for modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모든 데이터는 보통 지저분하고 데이터가 의도한 것에 대한 충분한 신뢰성을 가지고 있지 않다.\n",
    "\n",
    "데이터는 중복 데이터나 관찰되지 않은 값 (결측치), 아웃라이어, 존재하지 않는 주소, 잘못된 전화번호 또는 지역코드, 올바르지 않은 지역좌표, 잘못된 데이터나 레이블, 대소문자 구분, 공백 관련 문제를 가지고 있다.\n",
    "\n",
    "데이터과학자나 데이터엔지니어는 통계 모델 또는 머신러닝 모델을 빌드하기 위해 이러한 데이터를 깨끗하게 만들어야 한다.\n",
    "\n",
    "데이터는 앞에서 말한 문제점들이 없을 경우 기술적으로 깨끗하다고 말 할 수 있다.\n",
    "그러나 모델링 목적으로 데이터셋을 깨끗하게 하기 위해서는 피처의 분포를 확인해야 하고 사전에 정의된 조건들을 만족하는지 검증 해야 한다.\n",
    "\n",
    "- 데이터과학자는 80~90%의 시간을 데이터를 다루거나 피쳐에 익숙해지는데 쓰게 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark, pyspark\n",
    "findspark.find()\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "conf = pyspark.SparkConf().setAppName('appName').setMaster('local[*]')\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([\n",
    "    (1, 144.5, 5.9, 33, 'M'),\n",
    "    (2, 167.2, 5.4, 45, 'M'),\n",
    "    (3, 124.1, 5.2, 23, 'F'),\n",
    "    (4, 144.5, 5.9, 33, 'M'),\n",
    "    (5, 133.2, 5.7, 54, 'F'),\n",
    "    (3, 124.1, 5.2, 23, 'F'),\n",
    "    (5, 129.2, 5.3, 42, 'M'),\n",
    "], ['id', 'weight', 'height', 'age', 'gender'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count of rows: 7\n"
     ]
    }
   ],
   "source": [
    "print('count of rows: {0}'.format(df.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of distinct rows : 6\n"
     ]
    }
   ],
   "source": [
    "print('Count of distinct rows : {0}'.format(df.distinct().count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['weight', 'height', 'age', 'gender']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[c for c in df.columns if c != 'id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id', 'weight', 'height', 'age', 'gender']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+---+------+\n",
      "| id|weight|height|age|gender|\n",
      "+---+------+------+---+------+\n",
      "|  5| 133.2|   5.7| 54|     F|\n",
      "|  1| 144.5|   5.9| 33|     M|\n",
      "|  2| 167.2|   5.4| 45|     M|\n",
      "|  3| 124.1|   5.2| 23|     F|\n",
      "|  5| 129.2|   5.3| 42|     M|\n",
      "+---+------+------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.dropDuplicates(subset=[c for c in df.columns if c != 'id'])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+\n",
      "|count|distinct|\n",
      "+-----+--------+\n",
      "|    5|       4|\n",
      "+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.agg(fn.count('id').alias('count'), fn.countDistinct('id').alias('distinct')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "monotonically_increasing_id() <= 각 행에 고유한 값을 부연한다.\n",
    "각각의 파티션에 800억개의 데이터가 있고 파티션의 수가 약 10억개 미만인 데이터들에 대해서는 \n",
    "monotonically_increasing_id() 함수가 고유한 ID 값을 부여 해준다.\n",
    "\n",
    "** 주의 ** 스파크 이전 버전에서 monotonically_increasing_id() 함수는 같은 데이터프레임에서 여러번 작업이 이뤄졌을때만 ID값이 바뀌었는데\n",
    "스파크 2.0에서는 고정된 ID값을 부여한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+---+------+-------------+\n",
      "| id|weight|height|age|gender|       new_id|\n",
      "+---+------+------+---+------+-------------+\n",
      "|  5| 133.2|   5.7| 54|     F|  25769803776|\n",
      "|  1| 144.5|   5.9| 33|     M| 171798691840|\n",
      "|  2| 167.2|   5.4| 45|     M| 592705486848|\n",
      "|  3| 124.1|   5.2| 23|     F|1236950581248|\n",
      "|  5| 129.2|   5.3| 42|     M|1365799600128|\n",
      "+---+------+------+---+------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('new_id', fn.monotonically_increasing_id()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Missing observations\n",
    "- 미관찰 값들을 다룰 수 있는 가장 간단한 방법은 미관찰 값을 가지고있는 모든 데이터를 제거 하는 것이다.\n",
    "\n",
    "- 단, 너무 많은 데이터를 제거하지 않도록 조심해야함.\n",
    "- 데이터셋 전체에서 미관찰 값의 분포에 따라 데이터셋 전체의 사용 가능성에 큰 영향을 미칠 수 있다.\n",
    "- 데이터를 제거한 후 아주 작은 데이터만 남거나, 데이터가 절반 이상으로 줄었들었다면 어떤 피처가 빈칸을 가장 많이 가지고 있는지 확인하고 그 피처를 제거하는 것이 더 좋다.\n",
    "    - 데이터가 참/거짓으로 구분이되면 Missing라는 세번째 카테고리를 넣으면 된다.\n",
    "    - 데이터가 이미 카테고리를 가지고 있다면 Missing 카테고리를 집어 넣으면 된다.\n",
    "    - 순서 혹은 숫자 데이터를 가지고 있을 경우에는 평균, 중간값 또는 미리 정의된 다른 값으로 바꿀 수 있다. (ex: 데이터 분포에 따라 첫번째, 세번째 쿼타열 등 .....)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_miss = spark.createDataFrame( [ \n",
    "    (1, 144.5, 5.6, 28, 'M', 100000),\n",
    "    (2, 167.2, 5.4, 45, 'M', None),\n",
    "    (3, None , 5.2, None, None, None),\n",
    "    (4, 144.5, 5.9, 33, 'M', None),\n",
    "    (5, 133.2, 5.7, 54, 'F', None),\n",
    "    (6, 124.1, 5.2, None, 'F', None),\n",
    "    (7, 129.2, 5.3, 42, 'M', 76000) ],\n",
    "    ['id', 'weight', 'height', 'age', 'gender', 'income']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 0), (2, 1), (3, 4), (4, 1), (5, 1), (6, 2), (7, 0)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_miss.rdd.map(\n",
    "    lambda row : (row['id'], sum([c == None for c in row]))\n",
    ").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+----+------+------+\n",
      "| id|weight|height| age|gender|income|\n",
      "+---+------+------+----+------+------+\n",
      "|  3|  null|   5.2|null|  null|  null|\n",
      "+---+------+------+----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_miss.where('id==3').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------+-------------+------------------+------------------+------------------+\n",
      "|idmissing|     weightmissing|heightmissing|        agemissing|     gendermissing|     incomemissing|\n",
      "+---------+------------------+-------------+------------------+------------------+------------------+\n",
      "|      0.0|0.1428571428571429|          0.0|0.2857142857142857|0.1428571428571429|0.7142857142857143|\n",
      "+---------+------------------+-------------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_miss.agg(*[(1-(fn.count(c)/fn.count('*'))).alias(c + 'missing') for c in df_miss.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_miss_no_income = df_miss.select([c for c in df_miss.columns if c != 'income'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id', 'weight', 'height', 'age', 'gender', 'income']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_miss.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+----+------+\n",
      "| id|weight|height| age|gender|\n",
      "+---+------+------+----+------+\n",
      "|  1| 144.5|   5.6|  28|     M|\n",
      "|  2| 167.2|   5.4|  45|     M|\n",
      "|  4| 144.5|   5.9|  33|     M|\n",
      "|  5| 133.2|   5.7|  54|     F|\n",
      "|  6| 124.1|   5.2|null|     F|\n",
      "|  7| 129.2|   5.3|  42|     M|\n",
      "+---+------+------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_miss_no_income.dropna(thresh=3).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 미관찰값을 추정해 채우려면 fillna() 함수를 사용 할 수 있다.\n",
    "- 이 함수는 단일 integer, float, long, string 타입을 지원한다.\n",
    "- 전체 데이터셋에서 미관찰값은 그 값으로 채워질 것이다.\n",
    "\n",
    "- 평균, 중간값 또는 다른 계산된 값으로 채우려면 우선 그 값을 계산\n",
    "- 그러한 값을 가지는 딕셔너리를 만든 후 fillna() 함수에 전달 하면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = df_miss_no_income.agg(*[fn.mean(c).alias(c) for c in df_miss_no_income.columns if c != 'gender']).toPandas().to_dict('records')[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- toPandas() 함수는 collect()함수와 같은 방식으로 동작한다. 데이터 양에따라 문제가 발생 할 수 있다는 것을 기억해야 한다.\n",
    "- toPandas() 함수는 모든 정보를 워커 노드로부터 수집한 후, 드라이버 노드로 옮긴다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 4.0, 'weight': 140.45, 'height': 5.471428571428571, 'age': 40.4}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+---+------+\n",
      "| id|weight|height|age|gender|\n",
      "+---+------+------+---+------+\n",
      "|  1| 144.5|   5.6| 28|     M|\n",
      "|  2| 167.2|   5.4| 45|     M|\n",
      "|  3|140.45|   5.2| 40|  null|\n",
      "|  4| 144.5|   5.9| 33|     M|\n",
      "|  5| 133.2|   5.7| 54|     F|\n",
      "|  6| 124.1|   5.2| 40|     F|\n",
      "|  7| 129.2|   5.3| 42|     M|\n",
      "+---+------+------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_miss_no_income.fillna(means).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_outliers = spark.createDataFrame([\n",
    "    (1, 144.5, 5.3, 33,),\n",
    "    (2, 167.2, 5.5, 45,),\n",
    "    (3, 244.1, 5.1, 99,),\n",
    "    (4, 144.5, 5.5, 33,),\n",
    "    (5, 133.2, 5.4, 54,),\n",
    "    (6, 124.1, 5.1, 23,),\n",
    "    (7, 129.2, 5.3, 42,),\n",
    "], ['id', 'weight', 'height', 'age'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 아웃라이어는 대부분의 데이터와는 매우 다른 분포를 띄는 데이터를 말함\n",
    "- 매우 다르다는 것에 대한 정의는 각자 다를 수 있다.\n",
    "\n",
    "- ** 일반적으로 모든 값들이 대체적으로 Q1 - 1.5IQR 와 Q3 + 1.5IQR 사이에 있으면 아웃라이어가 없다고 말 할 수 있다.** \n",
    "\n",
    "- IQR은 상위 쿼타일(75%)과 하위 쿼타일(25%)의 차로 정의 된다.\n",
    "\n",
    "- approxQuantile() 함수를 사용 할 수 있다.\n",
    "- 첫번째 파라미터 : 컬럼명\n",
    "- 두번째 파라미터 : 0과 1사이 값 (0.5 중간값)\n",
    "- 세번째 파라미터 : 각 피처에 대한 허용 가능한 수준의 에러 (0으로 지정시 피처에 대한 정확한 값을 계산 할수 있으나 매우 많은 연산량을 요구한다\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['weight', 'height', 'age']\n",
    "bounds = {}\n",
    "\n",
    "for col in cols:\n",
    "    quantiles = df_outliers.approxQuantile(col, [0.25,0.75], 0.05)\n",
    "    IQR = quantiles[1] - quantiles[0]\n",
    "    bounds[col] = [quantiles[0] - 1.5 * IQR, quantiles[1] + 1.5 * IQR]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'weight': [72.19999999999999, 224.2],\n",
       " 'height': [4.499999999999999, 6.1000000000000005],\n",
       " 'age': [1.5, 85.5]}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers = df_outliers.select(\n",
    "    *['id'] + [\n",
    "        (\n",
    "            (df_outliers[c] < bounds[c][0]) | (df_outliers[c] > bounds[c][1])\n",
    "        ).alias(c + '_o') for c in cols\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+--------+-----+\n",
      "| id|weight_o|height_o|age_o|\n",
      "+---+--------+--------+-----+\n",
      "|  1|   false|   false|false|\n",
      "|  2|   false|   false|false|\n",
      "|  3|    true|   false| true|\n",
      "|  4|   false|   false|false|\n",
      "|  5|   false|   false|false|\n",
      "|  6|   false|   false|false|\n",
      "|  7|   false|   false|false|\n",
      "+---+--------+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "outliers.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_outliers = df_outliers.join(outliers, on = 'id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| id|age|\n",
      "+---+---+\n",
      "|  3| 99|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_outliers.filter('age_o').select('id', 'age').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id|weight|\n",
      "+---+------+\n",
      "|  3| 244.1|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_outliers.filter('weight_o').select('id', 'weight').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understand data\n",
    "\n",
    "- 기술통계\n",
    "- 기술통계는 데이터셋에서의 관찰 값 개수, 각 컬럼의 평균, 표준 편차 또는 최댓값/최솟값 등의 기본적인 정보를 확인한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.types as typ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이번 예제는 데이터가 커서 2개 cpu로는 부족\n",
    "- \\*로 해서 전체를 끌어서 사용했다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud = sc.textFile('./data/ccFraud.csv.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"custID\",\"gender\",\"state\",\"cardholder\",\"balance\",\"numTrans\",\"numIntlTrans\",\"creditLine\",\"fraudRisk\"'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "header = fraud.first()\n",
    "header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud = fraud\\\n",
    "        .filter(lambda row: row != header) \\\n",
    "        .map(lambda row: [int(elem) for elem in row.split(',')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = [\n",
    "    *[\n",
    "        typ.StructField(h[1:-1], typ.IntegerType(),True)\n",
    "        for h in header.split(',')\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = typ.StructType(fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud_df = spark.createDataFrame(fraud, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+\n",
      "|gender|  count|\n",
      "+------+-------+\n",
      "|     1|6178231|\n",
      "|     2|3821769|\n",
      "+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 빈도수 구하기\n",
    "fraud_df.groupby('gender').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# describe 확인\n",
    "# spark에서는 확인할 칼럼들을 적어주어야 한다.\n",
    "numerical = ['balance', 'numTrans', 'numIntlTrans']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc = fraud_df.describe(numerical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------------------+-----------------+\n",
      "|summary|          balance|          numTrans|     numIntlTrans|\n",
      "+-------+-----------------+------------------+-----------------+\n",
      "|  count|         10000000|          10000000|         10000000|\n",
      "|   mean|     4109.9199193|        28.9351871|        4.0471899|\n",
      "| stddev|3996.847309737077|26.553781024522852|8.602970115863767|\n",
      "|    min|                0|                 0|                0|\n",
      "|    max|            41485|               100|               60|\n",
      "+-------+-----------------+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "desc.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기술 통계는 작은 정보이지만 많은 것들을 알 수 있다. \n",
    "\n",
    "- 모든 피처는 양의 방향으로 왜곡이 됐다. (최댓값이 평균보다 몇 배 더 크다) \n",
    "- 변동 계수 coefficient variabioin (평균과 표준편차의 비율)가 매우 크다   \n",
    "    (값이 1과 가깝거나 크다, 이는 넓게 퍼진 데이터를 의미함) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "| skewness(balance)|\n",
      "+------------------+\n",
      "|1.1818315552995033|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#비대칭 확인 (balance 컬럼만 확인)\n",
    "fraud_df.agg({'balance':'skewness'}).show()\n",
    "# skewness -> 비대칭도\n",
    "# https://ko.wikipedia.org/wiki/%EB%B9%84%EB%8C%80%EC%B9%AD%EB%8F%84"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "편차 = X - M  \n",
    "표준 편차 ~ (X - M)^2  \n",
    "왜도 ~ (X - M)^3  \n",
    "첨도 ~ (X - M)^4  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlations\n",
    "corr() 함수는 두쌍의 상관계수만 계산할 수 있다. 피어슨 상관계수를 지원한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00044523140172659576"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "fraud_df.corr('balance', 'numTrans')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_numerical = len(numerical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = []\n",
    "for i in range(0, n_numerical):\n",
    "    temp = [None] * i\n",
    "    \n",
    "    for j in range(i, n_numerical):\n",
    "        temp.append(fraud_df.corr(numerical[i], numerical[j]))\n",
    "        corr.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1.0, 0.00044523140172659576, 0.00027139913398184604],\n",
       " [1.0, 0.00044523140172659576, 0.00027139913398184604],\n",
       " [1.0, 0.00044523140172659576, 0.00027139913398184604],\n",
       " [None, 1.0, -0.0002805712819816179],\n",
       " [None, 1.0, -0.0002805712819816179],\n",
       " [None, None, 1.0]]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# numerical = ['balance', 'numTrans', 'numIntlTrans']\n",
    "corr\n",
    "# 대립가설에 대한 상관계수가 낮다\n",
    "# => 쓸 수 있는 변수이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization\n",
    "conda install -c conda-forge bkcharts  \n",
    "conda install -c conda-forge holoviews\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram\n",
    "히스토그램은 피쳐의 분포를 시각화하는 가장 쉬운 방법이다.  \n",
    "파이스파크에서 히스토그램을 만드는 방법은 세가지!  \n",
    "\n",
    "- 데이터를 워커 노드에서 집계해서 워커 노드가 bin 리스트를 드라이버 노드에게 리턴하고 각 bin의 갯수를 드라이버 노드가 센다.\n",
    "- 데이터를 모두 드라이버 노드에 리턴, 시각화 라이브러리 함수를 사용해서 히스토그램을 만든다.\n",
    "- 데이터를 샘플링해서 드라이버 노드에 리턴, 드라이버 노드는 리턴된 데이터를 이용해 데이터를 시각화\n",
    "\n",
    "데이터의 행 수가 너무 많으면 두번쨰 방법은 아예 불가능!!!! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
